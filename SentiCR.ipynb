{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from SentiCR.SentiCR import SentiCR\n",
    "\n",
    "filename = '/Users/erdemdemir/DataspellProjects/empiricalSE/data/pythongeneral/Sep2020/pythongeneralSep2020.xml.out'\n",
    "df_original = pd.read_xml(filename)\n",
    "\n",
    "df = df_original[4:].drop(['team_domain', 'channel_name', 'start_date', 'end_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackOverflow Reading data from oracle..\n",
      "Training classifier model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erdemdemir/DataspellProjects/bert-arxiv/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ani', 'continu', 'deleg', 'doe', 'doubl', 'dure', 'els', 'endwhil', 'extend', 'implement', 'includ', 'interfac', 'namespac', 'nativ', 'nowwhil', 'onc', 'ourselv', 'overrid', 'packag', 'privat', 'protect', 'rais', 'readon', 'requir', 'sign', 'synchron', 'themselv', 'tri', 'veri', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training used 4.11 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "sentiment_analyzer=SentiCR()\n",
    "def predict_sentiCR(df, column = 'first_message'):\n",
    "    begin=time.time()\n",
    "    sentences=df[column]\n",
    "    pred=[]\n",
    "    for sent in sentences:\n",
    "        score=sentiment_analyzer.get_sentiment_polarity(sent)\n",
    "        pred.append(score)\n",
    "\n",
    "    end=time.time()\n",
    "    print('Prediction used {:.2f} seconds'.format(end-begin))\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction used 100.98 seconds\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "sentiCR_path = filename[-28:-8]+'-CR.csv'\n",
    "\n",
    "\n",
    "def extract(lst):\n",
    "    return [x[0] for x in lst]\n",
    "\n",
    "\n",
    "if exists(sentiCR_path):\n",
    "    df_sentiCR = pd.read_csv(sentiCR_path)\n",
    "else:\n",
    "    pred_full = predict_sentiCR(df, 'text')\n",
    "    df_sentiCR = df.copy()\n",
    "    df_sentiCR['sentiCR'] = extract(pred_full)\n",
    "    df_sentiCR['sentiCR'] = df_sentiCR['sentiCR'].replace({2: -1})\n",
    "    df_sentiCR.to_csv(sentiCR_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "df_sentiCR['sentiCR'].value_counts().to_csv(sentiCR_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}